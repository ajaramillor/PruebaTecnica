{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Importación de paquetes iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import logging\n",
    "import os\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Configuración del sistema de excepciones y logs\n",
    " Se van a almacenar en una carpeta en el root según las indicaciones de la prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un directorio de registros si no existe\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Ajustar la configuración de registro\n",
    "log_file = os.path.join(log_dir, \"application_limpieza.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(lineno)s - %(message)s\"\n",
    ")\n",
    "file_handler = logging.FileHandler(log_file, \"w\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(\n",
    "    logging.Formatter(\"%(asctime)s - %(levelname)s - %(lineno)s - %(message)s\")\n",
    ")\n",
    "logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "logging.debug(\"Prueba de mensaje de depuración\")\n",
    "logging.info(\"Prueba de mensaje informativo\")\n",
    "logging.warning(\"Prueba de mensaje de advertencia\")\n",
    "logging.error(\"Prueba de mensaje de error\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la ruta del archivo\n",
    "ruta_archivo = \"datasets/flights.csv\"\n",
    "\n",
    "# Lee el archivo CSV en un DataFrame de pandas, por velocidad se hace con una muestra de 400.000 filas\n",
    "df = pd.read_csv(ruta_archivo, delimiter=\"|\", nrows=400000)\n",
    "\n",
    "# Muestra las primeras filas del DataFrame\n",
    "df.head()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se comprueba que el dataset tiene más de un millon de filas como se pide en la prueba y que carga todas las filas del csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se revisa el tipo de cada columna para identificar si están correctas o se debe hacer algun ajuste\n",
    "df.info()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se identifica que la columna FlightDate deberia ser una fecha, la distancia debería ser un número, Cancelled y Diverted deben tener valor booleano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Se convierte la fecha\n",
    "    df[\"FLIGHTDATE\"] = pd.to_datetime(df[\"FLIGHTDATE\"], format=\"%Y%m%d\")\n",
    "\n",
    "    # Se ajusta la columna Distance\n",
    "    df[\"DISTANCE\"] = df[\"DISTANCE\"].str.replace(\" miles\", \"\")\n",
    "    df[\"DISTANCE\"] = pd.to_numeric(df[\"DISTANCE\"], errors=\"raise\")\n",
    "\n",
    "    df[\"DEPDELAY\"] = pd.to_timedelta(df[\"DEPDELAY\"], unit=\"min\", errors=\"ignore\")\n",
    "    df[\"TAXIOUT\"] = pd.to_timedelta(df[\"TAXIOUT\"], unit=\"min\", errors=\"ignore\")\n",
    "    df[\"TAXIIN\"] = pd.to_timedelta(df[\"TAXIIN\"], unit=\"min\", errors=\"ignore\")\n",
    "    df[\"ARRDELAY\"] = pd.to_timedelta(df[\"ARRDELAY\"], unit=\"min\", errors=\"ignore\")\n",
    "    df[\"CRSELAPSEDTIME\"] = pd.to_timedelta(\n",
    "        df[\"CRSELAPSEDTIME\"], unit=\"min\", errors=\"ignore\"\n",
    "    )\n",
    "    df[\"ACTUALELAPSEDTIME\"] = pd.to_timedelta(\n",
    "        df[\"ACTUALELAPSEDTIME\"], unit=\"min\", errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    # Se ajustan las columnas booleanas\n",
    "    df[\"CANCELLED\"] = df[\"CANCELLED\"].map(\n",
    "        {\"0\": False, \"1\": True, \"True\": True, \"False\": False, \"T\": True, \"F\": False}\n",
    "    )\n",
    "    df[\"DIVERTED\"] = df[\"DIVERTED\"].map(\n",
    "        {\"0\": False, \"1\": True, \"True\": True, \"False\": False, \"T\": True, \"F\": False}\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(f\"Se ha presentado una excepcion: {e}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Aunque no es del todo necesario capturas los errores en este punto pues es muy manual este proceso se deja para probar el sistema logging, si se corre dos veces este bloque se dispara una excepcion por aplicarle .str. a la columna distance que ya es numerica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Para seguir con las columnas que tiene formato de hora vemos que dos de ellas tienen formato int64 y el resto float64, quiere decir que las int no tienen valores nulos mientras que las otras si, se verifica esto para proceder a su transformació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir columnas a formato fecha hora\n",
    "columnas_hora = [\n",
    "    \"CRSDEPTIME\",\n",
    "    \"DEPTIME\",\n",
    "    \"WHEELSOFF\",\n",
    "    \"WHEELSON\",\n",
    "    \"CRSARRTIME\",\n",
    "    \"ARRTIME\",\n",
    "]\n",
    "for columna in columnas_hora:\n",
    "    if df[columna].isnull().sum() == 0:\n",
    "        df[columna] = df[columna].astype(str).str.zfill(4)\n",
    "        df[columna] = pd.to_datetime(\n",
    "            df[columna], errors=\"coerce\", format=\"%H%M\"\n",
    "        ).dt.time\n",
    "        df[columna] = (\n",
    "            df[\"FLIGHTDATE\"].dt.strftime(\"%Y-%m-%d\") + \" \" + df[columna].apply(str)\n",
    "        )\n",
    "        df[columna] = pd.to_datetime(df[columna], errors=\"coerce\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se verifican los cambios\n",
    "df.info()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores nulos\n",
    "null_values = df.isnull().sum() / len(df) * 100\n",
    "print(\"Porcentaje de valores nulos por columna:\")\n",
    "print(null_values)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " El porcentaje de datos faltantes en algunos campos va desde 0.9% hasta 15% en otros, en este punto del proceso no se considera eliminar estos registros, se procede a explorar los datos con el fin de decidir si descartar las columnas con alto porcentaje o si utilizar alguna técnica para completar la información faltante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\")\n",
    "plt.title(\"Matriz de correlación\")\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Aunque la matriz de correlación se utiliza más para modelos numéricos y preparar la data para modelos predictivos en este punto nos puede dar una pista si al descartar una columna podríamos afectar otra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Empezando por la columna que más datos le faltan (TAILNUM) se analiza si es necesario rellenar o no estos valores, esta columna corresponde a la identificación única de cada avión y sin más datos que nos permitan identificar a cada aeronave es imposible conseguir este valor; también se puede apreciar en la matriz que este valor no tiene gran influencia sobre las otras columnas, por lo tanto se dejaran los valores nulos y no se eliminaran estos registros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ahora se procede a revisar ORIGINSTATE y ORIGINSTATENAME, como tenemos el código del aeropuerto podemos buscarlo en los aeropuertos de destino y así encontrar la información faltante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario de mapeo entre DESTAIRPORTCODE y DESTSTATE\n",
    "codigos = df.set_index(\"DESTAIRPORTCODE\")[\"DESTSTATE\"].to_dict()\n",
    "\n",
    "# Rellenar los valores faltantes en ORIGINSTATE usando el diccionario de mapeo\n",
    "df[\"ORIGINSTATE\"] = df[\"ORIGINSTATE\"].fillna(df[\"ORIGINAIRPORTCODE\"].map(codigos))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Como esta estrategia no redujo el número de datos faltantes se procede a conectarse a una API gratuita que retorna la información del aeropuerto basado en el código único"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def airport_info(airport_code):\n",
    "    url = \"https://airport-info.p.rapidapi.com/airport\"\n",
    "\n",
    "    querystring = {\"iata\": airport_code}\n",
    "\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": \"6f294ed6f0mshd9fbb45d9c15ffbp112336jsn9ed08e8c22a6\",\n",
    "        \"X-RapidAPI-Host\": \"airport-info.p.rapidapi.com\",\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Se ha presentado una excepcion al consultar la API: {e}\")\n",
    "        return\n",
    "\n",
    "    print(response.json())\n",
    "\n",
    "    return response.json()\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar el diccionario para obtener solo las claves con valor NaN\n",
    "codigos_faltantes = {key: value for key, value in codigos.items() if pd.isna(value)}\n",
    "\n",
    "codigos_completos = {}\n",
    "\n",
    "for key in codigos_faltantes:\n",
    "    codigos_completos[key] = airport_info(key)[\"state\"]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Como ya se tiene un diccionario con el nombre del estado para cada aeropuerto faltante se agregan los datos al dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ORIGINSTATENAME\"] = df[\"ORIGINSTATENAME\"].fillna(\n",
    "    df[\"ORIGINAIRPORTCODE\"].map(codigos_completos)\n",
    ")\n",
    "df[\"DESTSTATENAME\"] = df[\"DESTSTATENAME\"].fillna(\n",
    "    df[\"DESTAIRPORTCODE\"].map(codigos_completos)\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ahora se completan las columnas ORIGINSTATE Y DESTSTATE, para esto se utiliza otro dataset pequeño con todos los estados y su abreviatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se carga el dataset\n",
    "df_estados = pd.read_csv(\"datasets/us_states.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Se extraen los estados y su abreviatura\n",
    "abreviaturas = df_estados.set_index(\"name\")[\"state\"].to_dict()\n",
    "\n",
    "# Se elimina el dataset para liberar memoria\n",
    "del df_estados\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ORIGINSTATE\"] = df[\"ORIGINSTATE\"].fillna(df[\"ORIGINSTATENAME\"].map(abreviaturas))\n",
    "df[\"DESTSTATE\"] = df[\"DESTSTATE\"].fillna(df[\"DESTSTATENAME\"].map(abreviaturas))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Columnas de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se comprueban si aún quedan valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores nulos\n",
    "null_values = df.isnull().sum() / len(df) * 100\n",
    "print(\"Porcentaje de valores nulos por columna:\")\n",
    "print(null_values)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Para los valores nulos que aún quedan se van a insertan los valores aproximados, por ejemplo hora de despegue programada y si la real es nula se pondrá la programada para poder rellenar los espacios, de todas formas como se vio al inicio estos valores solo corresponden al 2% y no presentan una cantidad estadisticamente considerable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir columnas a formato fecha hora\n",
    "columnas_hora = [\"DEPTIME\"]\n",
    "for columna in columnas_hora:\n",
    "    df[columna] = df[columna].astype(\"Int64\").astype(str)\n",
    "    df[columna] = df[columna].fillna(df[\"CRSDEPTIME\"])\n",
    "    df[columna] = df[columna].astype(str).str.zfill(4)\n",
    "    df[columna] = pd.to_datetime(df[columna], errors=\"coerce\", format=\"%H%M\").dt.time\n",
    "    df[columna] = (\n",
    "        df[\"FLIGHTDATE\"].dt.strftime(\"%Y-%m-%d\") + \" \" + df[columna].apply(str)\n",
    "    )\n",
    "    df[columna] = pd.to_datetime(df[columna], errors=\"coerce\")\n",
    "    df.loc[(df[columna].isnull()) & (df[\"DEPDELAY\"].isnull()), columna] = df[\n",
    "        \"CRSDEPTIME\"\n",
    "    ]\n",
    "    df.loc[(df[columna].isnull()) & (df[\"DEPDELAY\"].notnull()), columna] = (\n",
    "        df[\"CRSDEPTIME\"] + df[\"DEPDELAY\"]\n",
    "    )\n",
    "\n",
    "columnas_hora = [\"ARRTIME\"]\n",
    "for columna in columnas_hora:\n",
    "    df[columna] = df[columna].astype(\"Int64\").astype(str)\n",
    "    df[columna] = df[columna].astype(str).str.zfill(4)\n",
    "    df[columna] = pd.to_datetime(df[columna], errors=\"coerce\", format=\"%H%M\").dt.time\n",
    "    df[columna] = (\n",
    "        df[\"FLIGHTDATE\"].dt.strftime(\"%Y-%m-%d\") + \" \" + df[columna].apply(str)\n",
    "    )\n",
    "    df[columna] = pd.to_datetime(df[columna], errors=\"coerce\")\n",
    "    df.loc[(df[columna].isnull()) & (df[\"ARRDELAY\"].isnull()), columna] = df[\n",
    "        \"CRSARRTIME\"\n",
    "    ]\n",
    "    df.loc[(df[columna].isnull()) & (df[\"ARRDELAY\"].notnull()), columna] = (\n",
    "        df[\"CRSARRTIME\"] + df[\"ARRDELAY\"]\n",
    "    )\n",
    "\n",
    "\n",
    "columnas_hora = [\"WHEELSOFF\"]\n",
    "for columna in columnas_hora:\n",
    "    df[columna] = df[columna].astype(\"Int64\").astype(str)\n",
    "    df[columna] = df[columna].astype(str).str.zfill(4)\n",
    "    df[columna] = pd.to_datetime(df[columna], errors=\"coerce\", format=\"%H%M\").dt.time\n",
    "    df[columna] = (\n",
    "        df[\"FLIGHTDATE\"].dt.strftime(\"%Y-%m-%d\") + \" \" + df[columna].apply(str)\n",
    "    )\n",
    "    df[columna] = pd.to_datetime(df[columna], errors=\"coerce\")\n",
    "    df.loc[df[columna].isnull(), columna] = df[\"DEPTIME\"] + df[\"TAXIOUT\"]\n",
    "\n",
    "columnas_hora = [\"WHEELSON\"]\n",
    "for columna in columnas_hora:\n",
    "    df[columna] = df[columna].astype(\"Int64\").astype(str)\n",
    "    df[columna] = df[columna].astype(str).str.zfill(4)\n",
    "    df[columna] = pd.to_datetime(df[columna], errors=\"coerce\", format=\"%H%M\").dt.time\n",
    "    df[columna] = (\n",
    "        df[\"FLIGHTDATE\"].dt.strftime(\"%Y-%m-%d\") + \" \" + df[columna].apply(str)\n",
    "    )\n",
    "    df[columna] = pd.to_datetime(df[columna], errors=\"coerce\")\n",
    "    df.loc[df[columna].isnull(), columna] = df[\"ARRTIME\"] - df[\"TAXIIN\"]\n",
    "\n",
    "df[\"DEPDELAY\"] = df[\"DEPDELAY\"].dt.total_seconds() / 60\n",
    "df[\"TAXIOUT\"] = df[\"TAXIOUT\"].dt.total_seconds() / 60\n",
    "df[\"TAXIIN\"] = df[\"TAXIIN\"].dt.total_seconds() / 60\n",
    "df[\"ARRDELAY\"] = df[\"ARRDELAY\"].dt.total_seconds() / 60\n",
    "df[\"CRSELAPSEDTIME\"] = df[\"CRSELAPSEDTIME\"].dt.total_seconds() / 60\n",
    "df[\"ACTUALELAPSEDTIME\"] = df[\"ACTUALELAPSEDTIME\"].dt.total_seconds() / 60\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se corrigen las fechas de llegada para aquellos aviones que salen un día y llegan al siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arreglar_fecha(row):\n",
    "    if row[\"WHEELSOFF\"] < row[\"CRSDEPTIME\"]:\n",
    "        row[\"WHEELSOFF\"] = row[\"WHEELSOFF\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        row[\"WHEELSON\"] = row[\"WHEELSON\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        row[\"CRSARRTIME\"] = row[\"CRSARRTIME\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        row[\"ARRTIME\"] = row[\"ARRTIME\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        return row\n",
    "    elif row[\"WHEELSON\"] < row[\"WHEELSOFF\"]:\n",
    "        row[\"WHEELSON\"] = row[\"WHEELSON\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        row[\"CRSARRTIME\"] = row[\"CRSARRTIME\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        row[\"ARRTIME\"] = row[\"ARRTIME\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        return row\n",
    "    elif row[\"ARRTIME\"] < row[\"WHEELSON\"]:\n",
    "        row[\"CRSARRTIME\"] = row[\"CRSARRTIME\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        row[\"ARRTIME\"] = row[\"ARRTIME\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        return row\n",
    "    elif row[\"ARRTIME\"] < row[\"WHEELSOFF\"]:\n",
    "        row[\"ARRTIME\"] = row[\"ARRTIME\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        return row\n",
    "    elif row[\"CRSARRTIME\"] < row[\"WHEELSOFF\"]:\n",
    "        row[\"CRSARRTIME\"] = row[\"CRSARRTIME\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "        return row\n",
    "    elif row[\"ARRTIME\"] < row[\"DEPTIME\"]:\n",
    "        row[\"ARRTIME\"] = row[\"ARRTIME\"] + pd.to_timedelta(\n",
    "            1, unit=\"day\", errors=\"ignore\"\n",
    "        )\n",
    "\n",
    "    return row\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda row: arreglar_fecha(row), axis=1)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ya con las fechas ajustadas se procede a llenar los vacios en las columnas ACTUALELAPSEDTIME y CRSELAPSEDTIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"CRSELAPSEDTIME\"].isnull(), \"CRSELAPSEDTIME\"] = (\n",
    "    pd.to_timedelta(\n",
    "        df[\"CRSARRTIME\"] - df[\"CRSDEPTIME\"], unit=\"min\", errors=\"ignore\"\n",
    "    ).dt.total_seconds()\n",
    "    / 60\n",
    ")\n",
    "df.loc[df[\"ACTUALELAPSEDTIME\"].isnull(), \"ACTUALELAPSEDTIME\"] = (\n",
    "    pd.to_timedelta(\n",
    "        df[\"ARRTIME\"] - df[\"DEPTIME\"], unit=\"min\", errors=\"ignore\"\n",
    "    ).dt.total_seconds()\n",
    "    / 60\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se revisa ahora si quedaron vuelos con duración negativa, es posible si los datos originales están corruptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores negativos\n",
    "df.loc[\n",
    "    (df[\"ACTUALELAPSEDTIME\"] < 0) | (df[\"CRSELAPSEDTIME\"] < 0), \"ACTUALELAPSEDTIME\"\n",
    "].sum()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Todas las duraciones de los vuelos quedaron bien, sin embargo, con fines demostrativos se realiza un análisis de datos atípicos respecto a la duración y distancia de los para determinar si es necesarios eliminar algunos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficas de distribución de columnas numéricas\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, columna in enumerate([\"ACTUALELAPSEDTIME\", \"DISTANCE\"], 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.histplot(data=df, x=columna, kde=True)\n",
    "    plt.title(f\"Distribución de {columna}\")\n",
    "    plt.xlabel(columna)\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores atípicos\n",
    "atipicos = pd.DataFrame(columns=[\"Columna\", \"Valor atípico\"])\n",
    "\n",
    "# Para identificar los valores atipicos se utiliza el método del rango intercuartil, si conocieramos mejor el contexto\n",
    "# se podría escoger otro método como la media movil u otro método\n",
    "for columna in [\"ACTUALELAPSEDTIME\", \"DISTANCE\"]:\n",
    "    q1 = df[columna].quantile(0.25)\n",
    "    q3 = df[columna].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    limite_inferior = q1 - 1.5 * iqr\n",
    "    limite_superior = q3 + 1.5 * iqr\n",
    "\n",
    "    columna_atipica = df[\n",
    "        (df[columna] < limite_inferior) | (df[columna] > limite_superior)\n",
    "    ]\n",
    "    atipicos = pd.concat([atipicos, columna_atipica[[columna]]])\n",
    "\n",
    "# Se eliminan los valores atípicos\n",
    "df = df[~df.index.isin(atipicos.index)]\n",
    "\n",
    "print(\"Valores atípicos:\")\n",
    "print(atipicos)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Efectivamente los datos ya tienen una mejor distribución pero como se aclaró anteriormente esto se hace con fines demostrativos, en un caso real se debería tener el contexto de la información para entender estos valores atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficas de distribución de columnas numéricas\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, columna in enumerate([\"ACTUALELAPSEDTIME\", \"DISTANCE\"], 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.histplot(data=df, x=columna, kde=True)\n",
    "    plt.title(f\"Distribución de {columna}\")\n",
    "    plt.xlabel(columna)\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Con esto termina el proceso de limpieza del dataset para proceder con la carga y diseño de la BD relacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"datasets/flights_clean.parquet\", engine=\"auto\")\n",
    "del df\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " El dataset limpio se guarda en formato parquet que es más eficiente y pesa mucho menos. Se puede correr el notebook para generarlo nuevamente"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}